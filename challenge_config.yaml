# -----------------------------------------------------------------------------
# EvalAI Challenge Configuration for "Pinene: Soul of Dissonance"
# This file translates our bespoke Transmission Packet protocol and Pairwise
# Ranking Tournament into a standardized, public challenge format.
# -----------------------------------------------------------------------------

# --- Basic Challenge Information ---
# Sourced from the high-level project goals in the Transmission Packet.
title: "Pinene: Soul of Dissonance"
short_description: "An AI challenge to generate a creative-technical specification while adhering to a strict Fidelity Contract."
description: "templates/description.html" # You will need to create this HTML file.
evaluation_details: "templates/evaluation_details.html"
terms_and_conditions: "templates/terms_and_conditions.html"
image: "images/pinene_logo.png" # Placeholder for the challenge logo.
submission_guidelines: "templates/submission_guidelines.html"

# --- Evaluation Script ---
# This is the core of the challenge. It points to the script that will
# run the Round-Robin Tournament. Our local `2_run_tournament.py` is the
# prototype for this.
evaluation_script: "evaluation/judge.py"
remote_evaluation: True # Assumes evaluation will happen on the EvalAI backend.

# --- Timeline ---
# Using placeholders for a hypothetical launch next month.
start_date: "2025-10-01 12:00:00"
end_date: "2025-10-31 12:00:00"
published: True
tags: ["Generative AI", "Creative AI", "LLM Evaluation", "Agentic Workflow"]
domain: "NLP" # Natural Language Processing is the closest domain.

# --- Participation Rules ---
# Left blank to allow open participation.
allowed_email_domains: []
blocked_emails_domains: []

# --- Leaderboard Configuration ---
# This maps directly to our tournament results. The ranking is based on wins.
leaderboard:
  - id: 1
    schema:
      labels: ["Model", "Wins", "Losses", "Win_Percentage"]
      default_order_by: "Wins" # We rank models by the number of tournament wins.
      metadata:
        Wins:
          sort_ascending: False # Higher wins are better.
          description: "Total number of head-to-head matchups won in the round-robin tournament."
        Losses:
          sort_ascending: True # Lower losses are better.
          description: "Total number of head-to-head matchups lost."
        Win_Percentage:
          sort_ascending: False
          description: "The percentage of matchups won."

# --- Challenge Phases Configuration ---
# For this initial version, we will have one primary phase where participants
# submit their model's generated specification. Our evaluation script then
# runs the full tournament on all submissions to generate the leaderboard.
challenge_phases:
  - id: 1
    name: "Phase 1: Specification Tournament"
    description: "templates/phase_1_description.html"
    leaderboard_public: True
    is_public: True
    is_submission_public: True
    start_date: "2025-10-01 12:00:00"
    end_date: "2025-10-31 12:00:00"
    # The 'test_annotation_file' is our ground_truth_packet. It's the "question" for the exam.
    test_annotation_file: "annotations/ground_truth_packet_v3.json"
    codename: "phase_1_tournament"
    max_submissions_per_day: 5
    max_submissions: 10
    allowed_submission_file_types: [".txt"] # Participants submit their model's raw text output.

# --- Dataset and Split Configuration ---
# In our case, the "dataset" is the ground truth packet. We'll define one split.
dataset_splits:
  - id: 1
    name: "Primary Test Set"
    codename: "main_test"

# --- Mapping Phases to Datasets ---
# This links our single phase to our single dataset split and leaderboard.
challenge_phase_splits:
  - id: 1
    challenge_phase_id: 1
    leaderboard_id: 1
    dataset_split_id: 1
    visibility: 3 # 3 = Visible to everyone on leaderboard
    leaderboard_decimal_precision: 2
    is_leaderboard_order_descending: True # Order by wins, descending.
